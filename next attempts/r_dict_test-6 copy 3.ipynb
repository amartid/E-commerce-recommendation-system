{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 48.762688398361206\n",
      "Time elapsed: 75.98323082923889\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pathlib import Path # working with paths\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import bisect\n",
    "from itertools import product\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# Setting data paths with pathlib\n",
    "data_path = Path('/home/mai22042/otto/data')\n",
    "train_path = data_path/'train.jsonl'\n",
    "test_path = data_path/'test.jsonl'\n",
    "sample_sub_path = Path('sample_submission.csv')\n",
    "\n",
    "from enum import Enum\n",
    "class RName(Enum):\n",
    "    CLICK = 'clicks'\n",
    "    CART = 'carts'\n",
    "    ORDER = 'orders'\n",
    "\n",
    "    def __str__(self):\n",
    "        return '%s' % self.name\n",
    "    def __repr__(self):\n",
    "        return '%s' % self.name\n",
    "        \n",
    "# Set aliases\n",
    "CLICK=RName.CLICK\n",
    "CART=RName.CART\n",
    "ORDER=RName.ORDER\n",
    "\n",
    "# create a dict with key [aid,type], val (session)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class DT3:\n",
    "    def init_helper(self, chunk):\n",
    "        for session, events in zip(chunk['session'].tolist(), chunk['events'].tolist()):\n",
    "            for event in events:\n",
    "                key = event[\"aid\"], RName(event['type'])\n",
    "                if key not in self.products.keys():\n",
    "                    self.products[key] = set()\n",
    "                self.products[key].add(session)\n",
    "\n",
    "    def __init__(self, chunks, i=None, j=None, sampling=False):\n",
    "        self.products = {}\n",
    "        samples = []\n",
    "        if sampling:\n",
    "            samples = random.sample(range(34),1)\n",
    "            samples.sort()\n",
    "            print(samples)\n",
    "        for e, chunk in enumerate(chunks):\n",
    "            if (i==None and j==None) or (i<=e and j>e):\n",
    "                if sampling:\n",
    "                    if e in samples:\n",
    "                        print(\"test chunk number\",e)\n",
    "                        self.init_helper(chunk)\n",
    "                else:\n",
    "                    print(\"test chunk number\",e)\n",
    "                    self.init_helper(chunk)\n",
    "\n",
    "class DT2:\n",
    "    def init_helper(self, chunk):\n",
    "        for session, events in zip(chunk['session'].tolist(), chunk['events'].tolist()):\n",
    "            for event in events:\n",
    "                aid = event[\"aid\"]\n",
    "                key = session, RName(event['type'])\n",
    "                if key not in self.sessions.keys():\n",
    "                    self.sessions[key] = set()\n",
    "                self.sessions[key].add(aid)\n",
    "\n",
    "    def __init__(self, chunks, i=None, j=None, sampling=False):\n",
    "        self.sessions = {}\n",
    "        samples = []\n",
    "        if sampling:\n",
    "            samples = random.sample(range(34),1)\n",
    "            samples.sort()\n",
    "            print(samples)\n",
    "        for e, chunk in enumerate(chunks):\n",
    "            if (i==None and j==None) or (i<=e and j>e):\n",
    "                if sampling:\n",
    "                    if e in samples:\n",
    "                        print(\"test chunk number\",e)\n",
    "                        self.init_helper(chunk)\n",
    "                else:\n",
    "                    print(\"test chunk number\",e)\n",
    "                    self.init_helper(chunk)\n",
    "\n",
    "# Loading .jsonl file and creating DT object\n",
    "#start =time.time()\n",
    "#test_chunks  = pd.read_json(data_path / 'test.jsonl', lines=True, chunksize=400000)\n",
    "#test = DT3(test_chunks)\n",
    "#print (\"Time elapsed:\", time.time() - start)\n",
    "#del test_chunks # Delete chunks\n",
    "\n",
    "#                   CREATING TRAIN DT3\n",
    "\n",
    "# Loading .jsonl file and creating DT object\n",
    "# start =time.time()\n",
    "# train_chunks = pd.read_json(data_path / 'train.jsonl', lines=True, chunksize=400000)\n",
    "# train = DT3(train_chunks)\n",
    "# print (\"Time elapsed:\", time.time() - start)\n",
    "# del train_chunks # Delete chunks\n",
    "\n",
    "#                   SAVING TRAIN DT3\n",
    "\n",
    "# Saving object on pickle file\n",
    "\n",
    "# start =time.time()\n",
    "# with open('train_full_dt3.dat', 'wb') as f:\n",
    "#     pickle.dump(train, f)\n",
    "# print (\"Time elapsed:\", time.time() - start)\n",
    "\n",
    "\n",
    "\n",
    "#                   LOAD TRAIN DT3 FROM PICKLE FILE\n",
    "\n",
    "# Loading object from pickle file\n",
    "\n",
    "start =time.time()\n",
    "train = pickle.load(open(\"train_full_dt3.dat\", \"rb\"))\n",
    "print (\"Time elapsed:\", time.time() - start)\n",
    "\n",
    "#                   CREATING TRAIN DT2\n",
    "\n",
    "# Loading .jsonl file and creating DT object\n",
    "# start =time.time()\n",
    "# train_chunks = pd.read_json(data_path / 'train.jsonl', lines=True, chunksize=400000)\n",
    "# traindt2 = DT2(train_chunks)\n",
    "# print (\"Time elapsed:\", time.time() - start)\n",
    "# del train_chunks # Delete chunks\n",
    "#                   SAVING TRAIN DT3\n",
    "\n",
    "# Saving object on pickle file\n",
    "\n",
    "# start =time.time()\n",
    "# with open('train_full_dt2.dat', 'wb') as f:\n",
    "#     pickle.dump(traindt2, f)\n",
    "# print (\"Time elapsed:\", time.time() - start)\n",
    "\n",
    "#                   LOAD TRAIN DT2 FROM PICKLE FILE\n",
    "\n",
    "# Loading object from pickle file\n",
    "\n",
    "start =time.time()\n",
    "traindt2 = pickle.load(open(\"train_full_dt2.dat\", \"rb\"))\n",
    "print (\"Time elapsed:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 1.188821792602539\n"
     ]
    }
   ],
   "source": [
    "# Loading .jsonl file and creating DT object\n",
    "# start =time.time()\n",
    "# test_chunks  = pd.read_json(data_path / 'test.jsonl', lines=True, chunksize=400000)\n",
    "# test = DT3(test_chunks)\n",
    "# print (\"Time elapsed:\", time.time() - start)\n",
    "# del test_chunks # Delete chunks\n",
    "\n",
    "# start =time.time()\n",
    "# with open('test_dt3.dat', 'wb') as f:\n",
    "#     pickle.dump(test, f)\n",
    "# print (\"Time elapsed:\", time.time() - start)\n",
    "\n",
    "# Loading .jsonl file and creating DT object\n",
    "# start =time.time()\n",
    "# test_chunks  = pd.read_json(data_path / 'test.jsonl', lines=True, chunksize=400000)\n",
    "# testdt2 = DT2(test_chunks)\n",
    "# print (\"Time elapsed:\", time.time() - start)\n",
    "# del test_chunks # Delete chunks\n",
    "# Saving object on pickle file\n",
    "\n",
    "# start =time.time()\n",
    "# with open('test_dt2.dat', 'wb') as f:\n",
    "#     pickle.dump(testdt2, f)\n",
    "# print (\"Time elapsed:\", time.time() - start)\n",
    "\n",
    "start =time.time()\n",
    "testdt2 = pickle.load(open(\"test_dt2.dat\", \"rb\"))\n",
    "print (\"Time elapsed:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108125, 1460571, 29735, 184976, 95488, 1502122, 1733943, 959208, 322370, 231487, 756588, 832192, 554660, 801774, 1083665, 166037, 1603001, 673407, 1586171, 1196256, 620545, 332654, 819288, 199409, 1236775, 986164, 1645990, 102345, 794192, 1022566]\n",
      "dict_items([(108125, -1), (1460571, -2), (29735, -3), (184976, -4), (95488, -5), (1502122, -6), (1733943, -7), (959208, -8), (322370, -9), (231487, -10), (756588, -11), (832192, -12), (554660, -13), (801774, -14), (1083665, -15), (166037, -16), (1603001, -17), (673407, -18), (1586171, -19), (1196256, -20), (620545, -21), (332654, -22), (819288, -23), (199409, -24), (1236775, -25), (986164, -26), (1645990, -27), (102345, -28), (794192, -29), (1022566, -30)])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Sugest:\n",
    "    def __init__(self, list_of_sets, factor=1, weight=lambda x:x):\n",
    "        self.product_count = {}\n",
    "        self.factor=factor\n",
    "        # def radial_basis_gravity(distance = (1-d), where d=R, a):\n",
    "        self.weight=weight #lambda x,y=0.1:math.exp(-(1-x)**2 / y**2)\n",
    "        self.add_more(list_of_sets)\n",
    "\n",
    "    def add_more(self,list_of_sets, weight=lambda x:x):\n",
    "        self.weight=weight\n",
    "        # self.alpha=alpha\n",
    "        for i in list_of_sets:\n",
    "            if i!=None:\n",
    "                for p in i:\n",
    "                    self._add(p)\n",
    "\n",
    "    def add_one(self, set, factor, weight=lambda x:x):\n",
    "        self.factor=factor\n",
    "        self.weight=weight\n",
    "        # self.alpha=alpha\n",
    "        if set!=None:\n",
    "            for p in set:\n",
    "                self._add(p)\n",
    "\n",
    "    def reset_to_top(self):\n",
    "        self.product_count=self.backup.copy()\n",
    "\n",
    "    def _add(self, product):\n",
    "        if product in self.product_count.keys():\n",
    "            if self.product_count[product]<0:\n",
    "                self.product_count[product]=self.weight(self.factor)\n",
    "            else:\n",
    "                self.product_count[product]+=self.weight(self.factor)\n",
    "        else:\n",
    "            self.product_count[product]=self.weight(self.factor)\n",
    "\n",
    "    def top(self):\n",
    "        sorted_items = self.result(30)\n",
    "        tmp_dict={}\n",
    "        for e,i in enumerate(sorted_items):\n",
    "            tmp_dict[i]=int(-1-e)\n",
    "        self.product_count=tmp_dict\n",
    "        self.backup=tmp_dict.copy() #copy\n",
    "        \n",
    "\n",
    "    def result(self, number=20):\n",
    "        return  [\n",
    "            key for (key, value) in sorted(\n",
    "                self.product_count.items(), key=lambda x: x[1],reverse=True\n",
    "                )\n",
    "            ][:number]\n",
    "\n",
    "# get top products of train\n",
    "train_click_sets=[traindt2.sessions.get(tid) for tid in traindt2.sessions.keys() if tid[1]==CLICK]\n",
    "top_click=Sugest(train_click_sets)\n",
    "print(top_click.result(30))\n",
    "train_cart_sets=[traindt2.sessions.get(tid) for tid in traindt2.sessions.keys() if tid[1]==CART]\n",
    "top_cart=Sugest(train_cart_sets)\n",
    "train_order_sets=[traindt2.sessions.get(tid) for tid in traindt2.sessions.keys() if tid[1]==ORDER]\n",
    "top_order=Sugest(train_order_sets)\n",
    "\n",
    "top_click.top()\n",
    "top_cart.top()\n",
    "top_order.top()\n",
    "print(top_click.product_count.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(108125, -1), (1460571, -2), (29735, -3), (184976, -4), (95488, -5), (1502122, -6), (1733943, -7), (959208, -8), (322370, -9), (231487, -10), (756588, -11), (832192, -12), (554660, -13), (801774, -14), (1083665, -15), (166037, -16), (1603001, -17), (673407, -18), (1586171, -19), (1196256, -20), (620545, -21), (332654, -22), (819288, -23), (199409, -24), (1236775, -25), (986164, -26), (1645990, -27), (102345, -28), (794192, -29), (1022566, -30)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(top_click.product_count.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1671803/1671803 [12:15:55<00:00, 37.86it/s]   \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "def r_type(set_a, set_b):\n",
    "    if set_a==None or set_b==None:\n",
    "        return 0\n",
    "    if set_a == 0 or set_b == 0 :\n",
    "        return 0\n",
    "    intersection_len = len(set_a&set_b)\n",
    "    union_len = len(set_a|set_b)\n",
    "    if intersection_len == 0 or union_len == 0 :\n",
    "        return 0\n",
    "    else:\n",
    "        return intersection_len / union_len\n",
    "\n",
    "# headerList = ['session_type','labels']\n",
    "# with open(\"last\"+\".csv\", 'w') as file:\n",
    "#     dw = csv.DictWriter(file, delimiter=',', fieldnames=headerList)\n",
    "#     dw.writeheader()\n",
    "\n",
    "# rdict = []\n",
    "test_sessions=sorted(list(set([k[0] for k in testdt2.sessions.keys()])))\n",
    "# print([k[0] for k in testdt2.sessions.keys()][:10])\n",
    "# print(test_sessions[:10])\n",
    "for session in tqdm(test_sessions):\n",
    "    train_session_set=set()\n",
    "    test_product_click=testdt2.sessions.get((session,CLICK))\n",
    "    test_product_cart=testdt2.sessions.get((session,CART))\n",
    "    test_product_order=testdt2.sessions.get((session,ORDER))\n",
    "    if test_product_click is None:\n",
    "        test_product_click=set()\n",
    "    if test_product_cart is None:\n",
    "        test_product_cart=set()\n",
    "    if test_product_order is None:\n",
    "        test_product_order=set()\n",
    "    for aid in test_product_click:\n",
    "        key_train = aid,CLICK\n",
    "        if key_train in train.products.keys():\n",
    "            train_session_set|=train.products[key_train]\n",
    "    for aid in test_product_cart:\n",
    "        key_train = aid,CART\n",
    "        if key_train in train.products.keys():\n",
    "            train_session_set|=train.products[key_train]\n",
    "    for aid in test_product_order:\n",
    "        key_train = aid,ORDER\n",
    "        if key_train in train.products.keys():\n",
    "            train_session_set|=train.products[key_train]\n",
    "    rscores=[]\n",
    "    for session_train in train_session_set:\n",
    "        r_click=r_type(test_product_click,traindt2.sessions.get((session_train,CLICK)))\n",
    "        r_cart=r_type(test_product_cart,traindt2.sessions.get((session_train,CART)))\n",
    "        r_order=r_type(test_product_order,traindt2.sessions.get((session_train,ORDER)))\n",
    "        r_score=r_click*0.1+r_cart*0.3+r_order*0.6\n",
    "        rscores.append((session_train, r_click,r_cart,r_order,r_score))\n",
    "        \n",
    "        if len(rscores)>5000:\n",
    "            rscores=sorted(rscores, key= lambda x:x[4], reverse=True)[:1000]\n",
    "    rscores=sorted(rscores, key= lambda x:x[4], reverse=True)[:1000]\n",
    "    # print(session,rscores)\n",
    "    # rdict.append((session, rscores))\n",
    "    with open(\"rscores_1000\"+\".txt\", 'a+') as f:\n",
    "        f.write(str(session)+\" \")\n",
    "        for i in rscores:\n",
    "            f.write(str(i[0])+\" \"+str(i[1])+\" \"+str(i[2])+\" \"+str(i[3])+\" \"+str(i[4])+\" \")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of alpha: 0.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1671803it [04:06, 6778.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of alpha: 0.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1671803it [04:07, 6743.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import csv\n",
    "import numpy as np\n",
    "def get_set_length(s):\n",
    "    return len(s) if s else 0\n",
    "\n",
    "alpha = [0.5, 0.3, 0.108574, 0.1, 0.02] # list of alpha values\n",
    "for i, val in enumerate(alpha): # loop through the values\n",
    "    print(\"Value of alpha: {:.3f}\".format(val))\n",
    "    filename = \"ctc_ccto_20_rbf{:.3f}.csv\".format(val) # create a filename using alpha value    \n",
    "    headerList = ['session_type','labels']\n",
    "    with open(filename, 'w', newline='') as csvfile: # write headers \n",
    "        dw = csv.DictWriter(csvfile, delimiter=',', fieldnames=headerList)\n",
    "        dw.writeheader()\n",
    "\n",
    "    # def radial_basis_gravity(distance = (1-d), where d=R, a):\n",
    "    # weight=lambda x: math.sqrt(math.pow(x, 2) + math.pow(val, 2))\n",
    "    weight=lambda x:math.exp(-(1-x)**2 / val)\n",
    "    # weight= lambda x: np.sqrt(np.power(x, 2) + np.power(val, 2)) #sloww\n",
    "\n",
    "    # txt file contains 1000 nearest Rscores for every test session\n",
    "    with open('rscores_20.txt', 'r') as f:\n",
    "        # count=0\n",
    "        for line in tqdm(f):\n",
    "            # count+=1\n",
    "            # if count==5:\n",
    "            #     break\n",
    "            values = line.strip().split()\n",
    "            test_id = int(values[0])\n",
    "            top_cart.add_one( testdt2.sessions.get((test_id,CLICK)), 100,lambda x:x) #0.5 clicks to cart          \n",
    "            top_order.add_one(testdt2.sessions.get((test_id,CLICK)), 100,lambda x:x) #0.5 clicks to order\n",
    "            top_order.add_one(testdt2.sessions.get((test_id,CART)), 500,lambda x:x) #0.8 carts to order\n",
    "            for i in range(1, len(values), 5):\n",
    "                train_id = int(values[i])\n",
    "                # r_click = float(values[i+1])\n",
    "                # r_cart = float(values[i+2])\n",
    "                # r_order = float(values[i+3])\n",
    "                r_score = float(values[i+4])\n",
    "                # weight=lambda x:math.exp(-(1-x)**2 /get_set_length(traindt2.sessions.get((train_id,CLICK))))\n",
    "                # weight=lambda x: math.sqrt(math.pow(x, 2) + math.pow(get_set_length(traindt2.sessions.get((train_id,CLICK))), 2))\n",
    "                \n",
    "                top_click.add_one(traindt2.sessions.get((train_id,CLICK)), r_score, weight)\n",
    "                # weight=lambda x: math.sqrt(math.pow(x, 2) + math.pow(get_set_length(traindt2.sessions.get((train_id,CART))), 2))\n",
    "                \n",
    "                # weight=lambda x:math.exp(-(1-x)**2 / get_set_length(traindt2.sessions.get((train_id,CART))))\n",
    "                top_cart.add_one( traindt2.sessions.get((train_id,CART)),  r_score, weight)\n",
    "                # weight=lambda x: math.sqrt(math.pow(x, 2) + math.pow(get_set_length(traindt2.sessions.get((train_id,ORDER))), 2))\n",
    "\n",
    "                # weight=lambda x:math.exp(-(1-x)**2 / get_set_length(traindt2.sessions.get((train_id,ORDER))))\n",
    "                top_order.add_one(traindt2.sessions.get((train_id,ORDER)), r_score, weight)\n",
    "            sugest_click=top_click.result()\n",
    "            sugest_cart=top_cart.result()\n",
    "            sugest_order=top_order.result()            \n",
    "            top_click.reset_to_top()\n",
    "            top_cart.reset_to_top()\n",
    "            top_order.reset_to_top()            \n",
    "            with open(filename, 'a+') as h:\n",
    "                h.write(str(test_id)+\"_\"+CLICK.value+\",\"+\" \".join(map(str,sugest_click))+\"\\n\")\n",
    "                h.write(str(test_id)+\"_\"+CART.value+\",\"+\" \".join(map(str,sugest_cart))+\"\\n\")\n",
    "                h.write(str(test_id)+\"_\"+ORDER.value+\",\"+\" \".join(map(str,sugest_order))+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90912879824f678a927bbb77a5f5f39c19065948c0c15f812c2f0fa4c4f160c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
